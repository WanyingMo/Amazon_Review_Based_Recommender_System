{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prettytable as pt\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = \"../Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data about two large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalReviews = 157260921\n",
    "totalReviewPath = os.path.join(dataDir, \"All_Amazon_Review_5.json\")\n",
    "\n",
    "totalMeta = 15023060\n",
    "totalMetaPath = os.path.join(dataDir, \"All_Amazon_Meta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [30:48<00:00, 85.1kit/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overall': 'float', 'verified': 'bool', 'reviewTime': ('str', 1, 11), 'reviewerID': ('str', 11586, 20), 'asin': ('str', 0, 10), 'reviewerName': ('str', 28782337, 1725), 'reviewText': ('str', 140467823, 35094), 'summary': ('str', 151619415, 1730), 'unixReviewTime': 'int', 'vote': ('str', 257028, 6), 'image': ('list', 136488683, 508), 'style': ('dict', 2377009, 7)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# simple analysis on review dataset\n",
    "totalReviews = 157260921\n",
    "columns = ['verified', 'image', 'style', 'asin', 'reviewerID', 'overall', 'reviewText', 'reviewTime', 'unixReviewTime', 'summary', 'reviewerName', 'vote']\n",
    "column_dict = dict()\n",
    "style = set()\n",
    "dataPath = totalReviewPath\n",
    "with open(dataPath, 'r', encoding=\"utf-8\") as f:\n",
    "    with tqdm(total=totalReviews, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "        index = 0\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            pbar.update(1)\n",
    "            row = json.loads(line)\n",
    "            for key in row.keys():\n",
    "                valueType = type(row[key])\n",
    "                if key not in column_dict:\n",
    "                    if valueType in [list, dict, str]:\n",
    "                        column_dict[key] = (valueType.__name__, index, len(row[key]))\n",
    "                    else:\n",
    "                        column_dict[key] = (valueType.__name__)\n",
    "                else:\n",
    "                    if valueType in [list, dict, str] and len(row[key]) > column_dict[key][2]:\n",
    "                        column_dict[key] = (valueType.__name__, index, len(row[key]))\n",
    "            line = f.readline()\n",
    "            index += 1\n",
    "print(column_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 15.0M/15.0M [15:49<00:00, 15.8kit/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': ('list', 14000052, 171), 'tech1': ('str', 11042410, 25479), 'description': ('list', 5503049, 2253), 'fit': ('str', 14567591, 3701), 'title': ('str', 144162, 6234), 'also_buy': ('list', 1501, 100), 'image': ('list', 14728, 50), 'tech2': ('str', 7681322, 10348), 'brand': ('str', 7874918, 2000), 'feature': ('list', 14000052, 165), 'rank': ('str', 12046222, 1756), 'also_view': ('list', 58, 60), 'details': ('dict', 6534924, 12), 'main_cat': ('str', 1196634, 190), 'similar_item': ('str', 2289714, 84480), 'date': ('str', 10848403, 1264), 'price': ('str', 147, 4648), 'asin': ('str', 0, 10)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# simple analysis on review dataset\n",
    "totalMeta = 15023060\n",
    "column_dict = dict()\n",
    "style = set()\n",
    "dataPath = totalMetaPath\n",
    "with open(dataPath, 'r', encoding=\"utf-8\") as f:\n",
    "    with tqdm(total=totalMeta, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "        for index, line in enumerate(f):\n",
    "            row = json.loads(line)\n",
    "            for key in row.keys():\n",
    "                valueType = type(row[key])\n",
    "                if key not in column_dict:\n",
    "                    if valueType in [list, dict, str]:\n",
    "                        column_dict[key] = (valueType.__name__, index, len(row[key]))\n",
    "                    else:\n",
    "                        column_dict[key] = (valueType.__name__)\n",
    "                else:\n",
    "                    if valueType in [list, dict, str] and len(row[key]) > column_dict[key][2]:\n",
    "                        column_dict[key] = (valueType.__name__, index, len(row[key]))\n",
    "            pbar.update(1)\n",
    "print(column_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Review Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [23:15<00:00, 113kit/s]  \n"
     ]
    }
   ],
   "source": [
    "def splitColumns(selectedKeys:list, dataPath:str, totalRowNum:int, outputPath:str):\n",
    "    \"\"\"function used to split dataset\n",
    "\n",
    "    Args:\n",
    "        selectedKeys (list): list contains column names\n",
    "        dataPath (str): input file path\n",
    "        totalRowNum (int): number of rows\n",
    "        outputPath (str): output file path\n",
    "    \"\"\"\n",
    "    with open(outputPath, \"w\", newline=\"\") as outputFile:\n",
    "        writer = csv.DictWriter(outputFile, selectedKeys)\n",
    "        writer.writeheader()\n",
    "        with open(dataPath, encoding=\"utf-8\") as file:\n",
    "            with tqdm(total=totalRowNum, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "                for line in file:\n",
    "                    row = json.loads(line)\n",
    "                    writer.writerow(dict([(key, row.get(key)) for key in selectedKeys]))\n",
    "                    pbar.update()\n",
    "\n",
    "totalReviews = 157260921 # total number of reviews\n",
    "dataPath = \"../Data/All_Amazon_Review_5.json\" # input dataset path\n",
    "outputPath = \"../Data/All_Amazon_Review_User_Item_Rating.csv\" # output datset path\n",
    "selectedKeys = ['reviewerID', 'asin', 'overall'] # choose columns to split from dataset\n",
    "splitColumns(selectedKeys, dataPath, totalReviews, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset File: ../Data\\All_Amazon_Review_5.json\n",
      "Total Size: 157260921\n",
      "Selected Keys: ['reviewerID', 'asin', 'overall']\n",
      "Output File: ../Data\\All_Amazon_Review_User_Item_Rating.csv\n",
      "Number of processes: 12\n",
      "Size of each chunk: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 157M/157M [19:50<00:00, 132kit/s]    \n"
     ]
    }
   ],
   "source": [
    "import multiprocess_methods\n",
    "from multiprocessing import cpu_count\n",
    "if __name__ == '__main__':\n",
    "    dataPath = os.path.join(dataDir, \"All_Amazon_Review_5.json\")\n",
    "    totalSize = 157260921\n",
    "    selectedKeys = ['reviewerID', 'asin', 'overall']\n",
    "    outputPath = os.path.join(dataDir, \"All_Amazon_Review_User_Item_Rating.csv\")\n",
    "    processNum = cpu_count() # number of processes (customize this variable based on different CPUs)\n",
    "    chunkSize = 1024 # the size of each chunk splitted from the iterable\n",
    "    printParameters = True\n",
    "    multiprocess_methods.splitDataset(dataPath, totalSize, selectedKeys, outputPath, processNum, chunkSize, printParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 15.0M/15.0M [10:53<00:00, 23.0kit/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movies & tv', 'amazon fashion', 'sports & outdoors', 'amazon home', 'health & personal care', 'books', 'toys & games', 'baby', 'office products', 'all beauty', 'arts, crafts & sewing', 'digital music', 'video games', 'home audio & theater', 'tools & home improvement', 'all electronics', 'cell phones & accessories', 'camera & photo', 'industrial & scientific', 'musical instruments', 'automotive', 'computers', 'grocery', 'portable audio & accessories', 'software', 'car electronics', 'pet supplies', 'appliances', 'collectible coins', '', 'entertainment', 'luxury beauty', 'gps & navigation', 'amazon devices', 'buy a kindle', 'amazon launchpad', 'handmade', 'gift cards', 'sports collectibles', 'fine art', 'memberships & subscriptions', 'health &amp; personal care', 'arts, crafts &amp; sewing', 'toys &amp; games', 'industrial &amp; scientific', 'sports &amp; outdoors', 'tools &amp; home improvement', 'cell phones &amp; accessories', 'audible audiobooks', 'magazine subscriptions', 'camera &amp; photo', 'home audio &amp; theater', 'portable audio &amp; accessories', 'gps &amp; navigation', 'movies &amp; tv', 'fire phone', 'apple products', 'beats by dr. dre', 'amazon fire tv', 'collectibles & fine art', 'shorts', 'home & business services', 'prime pantry', '3d printing', 'vehicles', 'alexa skills']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "category_list = []\n",
    "\n",
    "dataPath = \"../Data/All_Amazon_Meta.json\"\n",
    "totalRowNum = 15023060\n",
    "\n",
    "with tqdm(total=totalRowNum, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "    with open(dataPath, 'r', encoding=\"utf-8\") as file:\n",
    "            for index, line in enumerate(file):\n",
    "                row = json.loads(line)\n",
    "                if row['main_cat'].lower() not in category_list:\n",
    "                    alt = re.search(\"alt\\s*=\\s*\\\"(.*)\\\"\", row['main_cat'].lower())\n",
    "                    if alt and alt.group(1) not in category_list:\n",
    "                        category_list.append(alt.group(1))\n",
    "                        # print(alt.group(1))\n",
    "                    elif not alt:\n",
    "                        category_list.append(row['main_cat'].lower())\n",
    "                        # print(row['main_cat'].lower())                             \n",
    "                pbar.update()\n",
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset File: ../Data/All_Amazon_Meta.json\n",
      "Total Size: 15023060\n",
      "Selected Keys: ['main_cat', 'asin', 'title']\n",
      "Output Directory: ../Data/Metadata/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 15.0M/15.0M [43:18<00:00, 5.78kit/s]  \n"
     ]
    }
   ],
   "source": [
    "def splitMetaColumns(category_dict:dict, selectedKeys:list, dataPath:str, totalRowNum:int, outputPath:str, printParameters:bool = False):\n",
    "    \"\"\"function used to split dataset\n",
    "\n",
    "    Args:\n",
    "        selectedKeys (list): list contains column names\n",
    "        dataPath (str): input file path\n",
    "        totalRowNum (int): number of rows\n",
    "        outputPath (str): output file path\n",
    "    \"\"\"\n",
    "    if printParameters:\n",
    "        # print(f\"Category List; {category_dict}\")\n",
    "        print(f\"Dataset File: {dataPath}\")\n",
    "        print(f\"Total Size: {totalRowNum}\")\n",
    "        print(f\"Selected Keys: {selectedKeys}\")\n",
    "        print(f\"Output Directory: {outputPath}\")\n",
    "\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.mkdir(outputPath)\n",
    "\n",
    "    for category in category_dict.keys():\n",
    "        fileName = category + \".csv\"\n",
    "        filePath = os.path.join(outputPath, fileName)\n",
    "        with open(filePath, \"w\", newline=\"\") as outputFile:\n",
    "            writer = csv.DictWriter(outputFile, selectedKeys)\n",
    "            writer.writeheader()\n",
    "    \n",
    "    with open(dataPath, encoding=\"utf-8\") as file:\n",
    "        with tqdm(total=totalRowNum, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "            for line in file:\n",
    "                row = json.loads(line)\n",
    "                rowDict = dict([(key, row.get(key)) for key in selectedKeys])\n",
    "                alt = re.search(\"alt\\s*=\\s*\\\"(.*)\\\"\", row['main_cat'].lower())\n",
    "                if alt:\n",
    "                    rowDict['main_cat'] = alt.group(1)\n",
    "                else:\n",
    "                    rowDict['main_cat'] = rowDict['main_cat'].lower()\n",
    "                \n",
    "                category = \"\"\n",
    "                for key, values in category_dict.items():\n",
    "                    if rowDict['main_cat'] in values:\n",
    "                        category = key\n",
    "                        break\n",
    "                if category == \"\":\n",
    "                    print(row['main_cat'])\n",
    "                    print(row['title'])\n",
    "                    break\n",
    "                with open(os.path.join(outputPath, category + \".csv\"), \"a\", newline=\"\") as outputFile:\n",
    "                    writer = csv.DictWriter(outputFile, selectedKeys)\n",
    "                    writer.writerow(rowDict)\n",
    "                pbar.update()\n",
    "\n",
    "category_dict = {\n",
    "    \"empty\": [\n",
    "        \"\"\n",
    "    ],\n",
    "    \"amazon product\": [\n",
    "        \"alexa skills\",\n",
    "        \"amazon devices\",\n",
    "        \"amazon fashion\",\n",
    "        \"amazon fire tv\",\n",
    "        \"amazon home\",\n",
    "        \"amazon launchpad\",\n",
    "        \"fire phone\",\n",
    "        \"handmade\",\n",
    "        \"memberships & subscriptions\"\n",
    "    ],\n",
    "    \"all beauty\": [\n",
    "        \"all beauty\"\n",
    "    ],\n",
    "    \"appliances\": [\n",
    "        \"appliances\"\n",
    "    ],\n",
    "    \"arts crafts and sewing\": [\n",
    "        \"arts, crafts & sewing\",\n",
    "        \"arts, crafts &amp; sewing\"\n",
    "    ],\n",
    "    \"automotive\": [\n",
    "        \"automotive\",\n",
    "        \"vehicles\"\n",
    "    ],\n",
    "    \"books\": [\n",
    "        \"audible audiobooks\",\n",
    "        \"books\"\n",
    "    ],\n",
    "    \"cell phones and accessories\": [\n",
    "        \"cell phones & accessories\",\n",
    "        \"cell phones &amp; accessories\"\n",
    "    ],\n",
    "    \"clothing shoes and jewelry\": [\n",
    "        \"shorts\"\n",
    "    ],\n",
    "    \"collectibles and fine art\": [\n",
    "        \"collectible coins\",\n",
    "        \"collectibles & fine art\",\n",
    "        \"fine art\",\n",
    "        \"sports collectibles\"\n",
    "    ],\n",
    "    \"digital music\": [\n",
    "        \"digital music\"\n",
    "    ],\n",
    "    \"electronics\": [\n",
    "        \"all electronics\",\n",
    "        \"apple products\",\n",
    "        \"beats by dr. dre\",\n",
    "        \"camera & photo\",\n",
    "        \"camera &amp; photo\",\n",
    "        \"car electronics\",\n",
    "        \"computers\",\n",
    "        \"gps & navigation\",\n",
    "        \"gps &amp; navigation\",\n",
    "        \"portable audio & accessories\",\n",
    "        \"portable audio &amp; accessories\"\n",
    "    ],\n",
    "    \"gift cards\": [\n",
    "        \"gift cards\"\n",
    "    ],\n",
    "    \"grocery and gourmet food\": [\n",
    "        \"grocery\"\n",
    "    ],\n",
    "    \"home and kitchen\": [\n",
    "        \"baby\",\n",
    "        \"health & personal care\",\n",
    "        \"health &amp; personal care\",\n",
    "        \"home & business services\",\n",
    "        \"home audio & theater\",\n",
    "        \"home audio &amp; theater\"\n",
    "    ],\n",
    "    \"industrial and scientific\": [\n",
    "        \"3d printing\",\n",
    "        \"industrial & scientific\",\n",
    "        \"industrial &amp; scientific\"\n",
    "    ],\n",
    "    \"kindle store\": [\n",
    "        \"buy a kindle\"\n",
    "    ],\n",
    "    \"luxury beauty\": [\n",
    "        \"luxury beauty\"\n",
    "    ],\n",
    "    \"magazine subscriptions\": [\n",
    "        \"magazine subscriptions\"\n",
    "    ],\n",
    "    \"movies and tv\": [\n",
    "        \"movies & tv\",\n",
    "        \"movies &amp; tv\"\n",
    "    ],\n",
    "    \"musical instruments\": [\n",
    "        \"musical instruments\"\n",
    "    ],\n",
    "    \"office products\": [\n",
    "        \"office products\"\n",
    "    ],\n",
    "    # \"patio lawn and garden\": [\n",
    "        \n",
    "    # ],\n",
    "    \"pet supplies\": [\n",
    "        \"pet supplies\"\n",
    "    ],\n",
    "    \"prime pantry\": [\n",
    "        \"prime pantry\"\n",
    "    ],\n",
    "    \"software\": [\n",
    "        \"software\"\n",
    "    ],\n",
    "    \"sports and outdoors\": [\n",
    "        \"sports & outdoors\",\n",
    "        \"sports &amp; outdoors\"\n",
    "    ],\n",
    "    \"tools and home improvement\": [\n",
    "        \"tools & home improvement\",\n",
    "        \"tools &amp; home improvement\"\n",
    "    ],\n",
    "    \"toys and games\": [\n",
    "        \"toys & games\",\n",
    "        \"toys &amp; games\"\n",
    "    ],\n",
    "    \"video games\": [\n",
    "        \"video games\"\n",
    "    ],\n",
    "    \"entertainment\":[\n",
    "        \"entertainment\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "totalMeta = 15023060\n",
    "dataPath = \"../Data/All_Amazon_Meta.json\" # input dataset path\n",
    "outputPath = \"../Data/Metadata/\" # output datset path\n",
    "selectedKeys = ['main_cat', 'asin', 'title'] # choose columns to split from dataset\n",
    "splitMetaColumns(category_dict, selectedKeys, dataPath, totalMeta, outputPath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verified</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>summary</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>B00UO1PFQO</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It will not attach as said in instructions, wi...</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>It will not attach as said in instructions, wi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>B00WA4AM7U</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great fit, like very much!</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>B00WA4AM7U</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great fit, like very much!</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>B00WA4AM7U</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great fit, like very much!</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>B00WA4AM7U</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It will not attach as said in instructions, wi...</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>It will not attach as said in instructions, wi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744394</th>\n",
       "      <td>True</td>\n",
       "      <td>B000V64P90</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love the band and love the song. I keep it o...</td>\n",
       "      <td>1384819200</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744395</th>\n",
       "      <td>True</td>\n",
       "      <td>B00BOW41P8</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I don't get a lot of free time so the papers j...</td>\n",
       "      <td>1384819200</td>\n",
       "      <td>get it now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744396</th>\n",
       "      <td>True</td>\n",
       "      <td>B0091LJ0IC</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love martinis but have never actually used t...</td>\n",
       "      <td>1381190400</td>\n",
       "      <td>my favorite</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744397</th>\n",
       "      <td>True</td>\n",
       "      <td>B001KBYPWW</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love them. They were what I was looking for....</td>\n",
       "      <td>1381190400</td>\n",
       "      <td>get what you pay for</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744398</th>\n",
       "      <td>True</td>\n",
       "      <td>0312360266</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I was up late one night and needed a new book ...</td>\n",
       "      <td>1340928000</td>\n",
       "      <td>Don't let your children read this</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744399 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         verified        asin            reviewerID  overall  \\\n",
       "0            True  B00UO1PFQO  A0038036HJMMW88H5SCN      1.0   \n",
       "1            True  B00WA4AM7U  A0038036HJMMW88H5SCN      5.0   \n",
       "2            True  B00WA4AM7U  A0038036HJMMW88H5SCN      5.0   \n",
       "3            True  B00WA4AM7U  A0038036HJMMW88H5SCN      5.0   \n",
       "4            True  B00WA4AM7U  A0038036HJMMW88H5SCN      1.0   \n",
       "...           ...         ...                   ...      ...   \n",
       "1744394      True  B000V64P90         AZZYKX2KZ0Q82      5.0   \n",
       "1744395      True  B00BOW41P8         AZZYKX2KZ0Q82      5.0   \n",
       "1744396      True  B0091LJ0IC         AZZYKX2KZ0Q82      5.0   \n",
       "1744397      True  B001KBYPWW         AZZYKX2KZ0Q82      5.0   \n",
       "1744398      True  0312360266         AZZYKX2KZ0Q82      4.0   \n",
       "\n",
       "                                                reviewText  unixReviewTime  \\\n",
       "0        It will not attach as said in instructions, wi...      1494633600   \n",
       "1                               Great fit, like very much!      1494633600   \n",
       "2                               Great fit, like very much!      1494633600   \n",
       "3                               Great fit, like very much!      1494633600   \n",
       "4        It will not attach as said in instructions, wi...      1494633600   \n",
       "...                                                    ...             ...   \n",
       "1744394  I love the band and love the song. I keep it o...      1384819200   \n",
       "1744395  I don't get a lot of free time so the papers j...      1384819200   \n",
       "1744396  I love martinis but have never actually used t...      1381190400   \n",
       "1744397  I love them. They were what I was looking for....      1381190400   \n",
       "1744398  I was up late one night and needed a new book ...      1340928000   \n",
       "\n",
       "                                                   summary  vote  \n",
       "0        It will not attach as said in instructions, wi...     4  \n",
       "1                                               Five Stars     0  \n",
       "2                                               Five Stars     0  \n",
       "3                                               Five Stars     0  \n",
       "4        It will not attach as said in instructions, wi...     4  \n",
       "...                                                    ...   ...  \n",
       "1744394                                          Loved it!     0  \n",
       "1744395                                         get it now     0  \n",
       "1744396                                        my favorite     0  \n",
       "1744397                               get what you pay for     0  \n",
       "1744398                  Don't let your children read this     0  \n",
       "\n",
       "[1744399 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../Data/train_100k.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90366</th>\n",
       "      <td>AMXE0HOAV0R9I</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91512</th>\n",
       "      <td>AOJ9OER7G5Q3C</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15023</th>\n",
       "      <td>A1KHTM1WYLVXAA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66355</th>\n",
       "      <td>A3I3OQF4FYR1SZ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81941</th>\n",
       "      <td>AB7W9FZ5O7CVJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99819</th>\n",
       "      <td>AZQS2CHMYKQ6V</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99820</th>\n",
       "      <td>AZQY5M5WGD7MB</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99859</th>\n",
       "      <td>AZT2XCDE2SGUL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99890</th>\n",
       "      <td>AZUU1U55E1A3A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>AZZ1YNB1QGCZ6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID  mean  count\n",
       "90366   AMXE0HOAV0R9I   1.0     19\n",
       "91512   AOJ9OER7G5Q3C   1.0     15\n",
       "15023  A1KHTM1WYLVXAA   1.0     14\n",
       "66355  A3I3OQF4FYR1SZ   1.0     14\n",
       "81941   AB7W9FZ5O7CVJ   1.0     12\n",
       "...               ...   ...    ...\n",
       "99819   AZQS2CHMYKQ6V   5.0      7\n",
       "99820   AZQY5M5WGD7MB   5.0      7\n",
       "99859   AZT2XCDE2SGUL   5.0      7\n",
       "99890   AZUU1U55E1A3A   5.0      7\n",
       "99977   AZZ1YNB1QGCZ6   5.0      7\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df.groupby('reviewerID', as_index=False)['overall'].agg(['mean', 'count']).reset_index().sort_values(by=['mean', 'count'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 30.0/30.0 [00:36<00:00, 1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# join_df = pd.merge(train_df, all_beauty_df, how=\"left\", on=['asin'])\n",
    "# join_df.loc[join_df['main_cat'].isna() == False]\n",
    "metadata_df = pd.DataFrame()\n",
    "fileNames = os.listdir(\"../Data/Metadata\")\n",
    "with tqdm(total=len(fileNames), desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "    for fileName in fileNames:\n",
    "        primary_category = fileName.split(\".\")[0]\n",
    "        temp_df = pd.read_csv(\"../Data/Metadata/\" + fileName)\n",
    "        temp_df['primary_cat'] = primary_category\n",
    "        metadata_df = pd.concat([metadata_df, temp_df])\n",
    "        pbar.update()\n",
    "metadata_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.drop_duplicates(inplace=True)\n",
    "metadata_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verified</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>summary</th>\n",
       "      <th>vote</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>title</th>\n",
       "      <th>primary_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>B00UO1PFQO</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It will not attach as said in instructions, wi...</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>It will not attach as said in instructions, wi...</td>\n",
       "      <td>4</td>\n",
       "      <td>automotive</td>\n",
       "      <td>Husky Liners Under Seat Storage Box Fits 15-18...</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>B00WA4AM7U</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great fit, like very much!</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>0</td>\n",
       "      <td>automotive</td>\n",
       "      <td>Husky Liners 2nd Seat Floor Liner Fits 15-18 F...</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>B00WA4AM7U</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great fit, like very much!</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>0</td>\n",
       "      <td>automotive</td>\n",
       "      <td>Husky Liners 2nd Seat Floor Liner Fits 15-18 F...</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>B00WA4AM7U</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great fit, like very much!</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>0</td>\n",
       "      <td>automotive</td>\n",
       "      <td>Husky Liners 2nd Seat Floor Liner Fits 15-18 F...</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>B00WA4AM7U</td>\n",
       "      <td>A0038036HJMMW88H5SCN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It will not attach as said in instructions, wi...</td>\n",
       "      <td>1494633600</td>\n",
       "      <td>It will not attach as said in instructions, wi...</td>\n",
       "      <td>4</td>\n",
       "      <td>automotive</td>\n",
       "      <td>Husky Liners 2nd Seat Floor Liner Fits 15-18 F...</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744394</th>\n",
       "      <td>True</td>\n",
       "      <td>B000V64P90</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love the band and love the song. I keep it o...</td>\n",
       "      <td>1384819200</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744395</th>\n",
       "      <td>True</td>\n",
       "      <td>B00BOW41P8</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I don't get a lot of free time so the papers j...</td>\n",
       "      <td>1384819200</td>\n",
       "      <td>get it now</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744396</th>\n",
       "      <td>True</td>\n",
       "      <td>B0091LJ0IC</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love martinis but have never actually used t...</td>\n",
       "      <td>1381190400</td>\n",
       "      <td>my favorite</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon home</td>\n",
       "      <td>Rabbit Electric Cocktail Mixer (18-Ounce)</td>\n",
       "      <td>amazon product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744397</th>\n",
       "      <td>True</td>\n",
       "      <td>B001KBYPWW</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love them. They were what I was looking for....</td>\n",
       "      <td>1381190400</td>\n",
       "      <td>get what you pay for</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon home</td>\n",
       "      <td>Yamazaki Stainless Steel My Chopsticks in Silv...</td>\n",
       "      <td>amazon product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744398</th>\n",
       "      <td>True</td>\n",
       "      <td>0312360266</td>\n",
       "      <td>AZZYKX2KZ0Q82</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I was up late one night and needed a new book ...</td>\n",
       "      <td>1340928000</td>\n",
       "      <td>Don't let your children read this</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "      <td>Marked (House of Night, Book 1)</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744399 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         verified        asin            reviewerID  overall  \\\n",
       "0            True  B00UO1PFQO  A0038036HJMMW88H5SCN      1.0   \n",
       "1            True  B00WA4AM7U  A0038036HJMMW88H5SCN      5.0   \n",
       "2            True  B00WA4AM7U  A0038036HJMMW88H5SCN      5.0   \n",
       "3            True  B00WA4AM7U  A0038036HJMMW88H5SCN      5.0   \n",
       "4            True  B00WA4AM7U  A0038036HJMMW88H5SCN      1.0   \n",
       "...           ...         ...                   ...      ...   \n",
       "1744394      True  B000V64P90         AZZYKX2KZ0Q82      5.0   \n",
       "1744395      True  B00BOW41P8         AZZYKX2KZ0Q82      5.0   \n",
       "1744396      True  B0091LJ0IC         AZZYKX2KZ0Q82      5.0   \n",
       "1744397      True  B001KBYPWW         AZZYKX2KZ0Q82      5.0   \n",
       "1744398      True  0312360266         AZZYKX2KZ0Q82      4.0   \n",
       "\n",
       "                                                reviewText  unixReviewTime  \\\n",
       "0        It will not attach as said in instructions, wi...      1494633600   \n",
       "1                               Great fit, like very much!      1494633600   \n",
       "2                               Great fit, like very much!      1494633600   \n",
       "3                               Great fit, like very much!      1494633600   \n",
       "4        It will not attach as said in instructions, wi...      1494633600   \n",
       "...                                                    ...             ...   \n",
       "1744394  I love the band and love the song. I keep it o...      1384819200   \n",
       "1744395  I don't get a lot of free time so the papers j...      1384819200   \n",
       "1744396  I love martinis but have never actually used t...      1381190400   \n",
       "1744397  I love them. They were what I was looking for....      1381190400   \n",
       "1744398  I was up late one night and needed a new book ...      1340928000   \n",
       "\n",
       "                                                   summary  vote     main_cat  \\\n",
       "0        It will not attach as said in instructions, wi...     4   automotive   \n",
       "1                                               Five Stars     0   automotive   \n",
       "2                                               Five Stars     0   automotive   \n",
       "3                                               Five Stars     0   automotive   \n",
       "4        It will not attach as said in instructions, wi...     4   automotive   \n",
       "...                                                    ...   ...          ...   \n",
       "1744394                                          Loved it!     0          NaN   \n",
       "1744395                                         get it now     0          NaN   \n",
       "1744396                                        my favorite     0  amazon home   \n",
       "1744397                               get what you pay for     0  amazon home   \n",
       "1744398                  Don't let your children read this     0        books   \n",
       "\n",
       "                                                     title     primary_cat  \n",
       "0        Husky Liners Under Seat Storage Box Fits 15-18...      automotive  \n",
       "1        Husky Liners 2nd Seat Floor Liner Fits 15-18 F...      automotive  \n",
       "2        Husky Liners 2nd Seat Floor Liner Fits 15-18 F...      automotive  \n",
       "3        Husky Liners 2nd Seat Floor Liner Fits 15-18 F...      automotive  \n",
       "4        Husky Liners 2nd Seat Floor Liner Fits 15-18 F...      automotive  \n",
       "...                                                    ...             ...  \n",
       "1744394                                                NaN             NaN  \n",
       "1744395                                                NaN             NaN  \n",
       "1744396          Rabbit Electric Cocktail Mixer (18-Ounce)  amazon product  \n",
       "1744397  Yamazaki Stainless Steel My Chopsticks in Silv...  amazon product  \n",
       "1744398                    Marked (House of Night, Book 1)           books  \n",
       "\n",
       "[1744399 rows x 11 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_df = pd.merge(train_df, metadata_df, how=\"left\", on='asin')\n",
    "join_df\n",
    "# train_df.join(metadata_df.set_index('asin'), on='asin')\n",
    "# metadata_df.asin.value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10422"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(join_df.loc[join_df['main_cat'].isna() == True]['asin'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2.50M/2.50M [01:53<00:00, 22.0kit/s]\n"
     ]
    }
   ],
   "source": [
    "def sampleData(sampleSize:int, totalSize:int, dataPath:str, outputPath:str):\n",
    "    \"\"\"function used to sample a small dataset from a large csv file\n",
    "\n",
    "    Args:\n",
    "        sampleSize (int): the size of the sample\n",
    "        totalSize (int): the size of the original dataset\n",
    "        dataPath (str): original dataset path\n",
    "        outputPath (str): sample dataset output path\n",
    "    \"\"\"\n",
    "    sampleIndicesList = sorted(random.sample(range(totalSize), sampleSize))\n",
    "    sampleIndex = 0\n",
    "    with open(dataPath, encoding=\"utf-8\", newline='') as file:\n",
    "        csvReader = csv.reader(file)\n",
    "        csvHeader = next(csvReader) # read header\n",
    "        with open(outputPath, \"w\", newline='') as outputFile:\n",
    "            csvWriter = csv.writer(outputFile, csvHeader)\n",
    "            csvWriter.writerow(csvHeader) # write header\n",
    "            with tqdm(total=sampleSize, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "                for index, row in enumerate(csvReader):\n",
    "                    if index == sampleIndicesList[sampleIndex]:\n",
    "                        csvWriter.writerow(row)\n",
    "                        sampleIndex += 1\n",
    "                        pbar.update()\n",
    "                    if sampleIndex == sampleSize:\n",
    "                        break\n",
    "                    \n",
    "\n",
    "data_dir = \"../Data/\"\n",
    "input_csv_path = os.path.join(data_dir, \"All_Amazon_Review_User_Item_Rating.csv\")\n",
    "output_csv_path = os.path.join(data_dir, \"sampled_data.csv\")\n",
    "total_records = 157260921\n",
    "required_records = 2500000\n",
    "sampleData(required_records, total_records, input_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset File: ../Data/All_Amazon_Review_User_Item_Rating.csv\n",
      "Total Size: 157260921\n",
      "Output File: ../Data/sampled_data.csv\n",
      "Sample size: 2500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2.50M/2.50M [01:59<00:00, 20.8kit/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocess_methods\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = \"../Data/\"\n",
    "    input_csv_path = os.path.join(data_dir, \"All_Amazon_Review_User_Item_Rating.csv\")\n",
    "    output_csv_path = os.path.join(data_dir, \"sampled_data.csv\")\n",
    "    total_records = 157260921\n",
    "    required_records = 2500000\n",
    "    printParameters = True\n",
    "    multiprocess_methods.sampleDataset(input_csv_path, total_records, output_csv_path, required_records, printParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data directly from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 12.5M/12.5M [26:06<00:00, 7.98kit/s]  \n"
     ]
    }
   ],
   "source": [
    "def sampleDataJSON(sampleSize:int, totalSize:int, dataPath:str, outputPath:str, columns:list):\n",
    "    \"\"\"function used to sample a small csv dataset from a large JSON file\n",
    "\n",
    "    Args:\n",
    "        sampleSize (int): the size of the sample\n",
    "        totalSize (int): the size of the original dataset\n",
    "        dataPath (str): original dataset path\n",
    "        outputPath (str): sample dataset output path\n",
    "        columns (list): list of cloumn required in the sampled dataset\n",
    "    \"\"\"\n",
    "    sampleIndicesList = sorted(random.sample(range(totalSize), sampleSize))\n",
    "    sampleIndex = 0\n",
    "\n",
    "    with open(outputPath, \"w\", newline=\"\") as outputFile:\n",
    "        writer = csv.DictWriter(outputFile, columns)\n",
    "        writer.writeheader()\n",
    "        with open(dataPath, encoding=\"utf-8\") as file:\n",
    "            with tqdm(total=sampleSize, desc=\"Processing\", leave=True, unit_scale=True) as pbar:\n",
    "                for index, line in enumerate(file):\n",
    "                    if index == sampleIndicesList[sampleIndex]:\n",
    "                        row = json.loads(line)\n",
    "                        writer.writerow(dict([(key, row.get(key, \"\")) for key in columns]))\n",
    "                        sampleIndex += 1\n",
    "                        pbar.update()\n",
    "                    if sampleIndex == sampleSize:\n",
    "                        break\n",
    "                    \n",
    "\n",
    "data_dir = \"../Data/\"\n",
    "input_json_path = os.path.join(data_dir, \"All_Amazon_Review_5.json\")\n",
    "output_csv_path = os.path.join(data_dir, \"sampled_data.csv\")\n",
    "total_records = 157260921\n",
    "required_records = 12500000\n",
    "columns = ['verified', 'image', 'style', 'asin', 'reviewerID', 'overall', 'reviewText', 'reviewTime', 'summary', 'vote']\n",
    "sampleDataJSON(required_records, total_records, input_json_path, output_csv_path, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12477503 entries, 0 to 12499999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   verified    bool   \n",
      " 1   image       object \n",
      " 2   style       object \n",
      " 3   asin        object \n",
      " 4   reviewerID  object \n",
      " 5   overall     float64\n",
      " 6   reviewText  object \n",
      " 7   reviewTime  object \n",
      " 8   summary     object \n",
      " 9   vote        object \n",
      "dtypes: bool(1), float64(1), object(8)\n",
      "memory usage: 963.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/sampled_data.csv\", low_memory=False)\n",
    "df.dropna(how=\"all\", inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Validation - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Data\"\n",
    "\n",
    "# Shuffle entire DataFrame and reset index before splitting\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "total_records = len(df)\n",
    "\n",
    "# Take first 60% of rows as training data\n",
    "df_train = df.iloc[ : int(total_records * 0.6), :]\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_train.to_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "\n",
    "# Take next 20% of rows as validation data\n",
    "df_validation = df.iloc[int(total_records * 0.6) : int(total_records * 0.8), :]\n",
    "df_validation.reset_index(inplace=True, drop=True)\n",
    "df_validation.to_csv(os.path.join(data_dir, \"validation.csv\"))\n",
    "\n",
    "# Take the last 20% of rows as test data\n",
    "df_test = df.iloc[int(total_records * 0.8) : , :]\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "df_test.to_csv(os.path.join(data_dir, \"test.csv\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcf1d46d271c46101d6967829d4a5f475342a2ce08e4944f989fbcdc9bb23690"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
